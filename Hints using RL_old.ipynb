{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Hints using RL_old.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HMl3akC8pe4",
        "colab_type": "code",
        "colab": {},
        "outputId": "de02ac45-d4ff-4d3d-88c4-e846450f314a"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ0FjjDI8pe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import queue\n",
        "import os, sys, time, json, random, string\n",
        "import numpy as np\n",
        "import socket\n",
        "import datetime as dt\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "random.seed(101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojcE18FL8pe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Maze and items and other global parametersaaaaaaaaaaaaaaaaa\n",
        "\n",
        "maze1 = np.asarray([\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0],\n",
        "    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],])\n",
        "\n",
        "ITEMS_LOC = [(6,6),(0,8),(0,0),[8,0]]\n",
        "ITEMS = ['la lavende', 'les noix de coco','les pommes',  'inventory']\n",
        "START_LOC = [8, 0]\n",
        "EMOTIONS_LIST = ['F', 'E', 'EN']\n",
        "N_EPOCH = 1000\n",
        "HINTS = [0,1]\n",
        "ACTIONS_DICT = {\n",
        "    0: 'do nothing',\n",
        "    0: 'left',\n",
        "    1: 'up',\n",
        "    2: 'right',\n",
        "    3: 'down'\n",
        "}\n",
        "GAMMA = 0.95\n",
        "MAX_MEMORY = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_oVlKpv8pfC",
        "colab_type": "code",
        "colab": {},
        "outputId": "6121b984-b8c0-48fd-cb65-870c6f6bae37"
      },
      "source": [
        "plt.imshow(maze1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x11aa24541c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9a42M8x8pfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, maze, items_loc, human, hint = 0):\n",
        "        self._maze = np.array(maze)\n",
        "        self.human = human\n",
        "        self.human_loc = START_LOC\n",
        "        self._targets = target_list\n",
        "        self.free_cells = self.get_free_cells()\n",
        "        self.blocked_cells = self.get_blocked_cells()\n",
        "        self.hint_on_display = 0\n",
        "        self.reset(human, hint)\n",
        "        \n",
        "    def reset(self, human,  hint= 0):\n",
        "        self.human.reset()\n",
        "        self.human_loc = START_LOC\n",
        "        self.hint = hint\n",
        "        self.hint_on_display = 0 #False\n",
        "        self.time = 0\n",
        "        self.maze = np.copy(self._maze)\n",
        "        self.targets = set(self._targets)\n",
        "        row,col = self.human_loc\n",
        "        self.maze[row, col] = human_mark\n",
        "        self.state = (self.human_loc, self.human.emotions, self.hint_on_display)\n",
        "        self.total_reward = 0 #Total return of episode\n",
        "        self.time_since_last_hint = 0.0001 #To avoid divide by zero\n",
        "        self.base = np.sqrt(self.maze.size)\n",
        "        self.min_reward =  - self.maze.size/len(self._targets)\n",
        "        self.reward = {\n",
        "            'targets'           :  1.0/len(self._targets),\n",
        "            'hint_penalty'      : -len(self._targets) /self.base,\n",
        "            'time_penalty'      : -1/self.maze.size,\n",
        "            'hint_time_penalty' : -1/self.time_since_last_hint\n",
        "        }\n",
        "        \n",
        "        nrows, ncols = self.maze.shape\n",
        "        self.visited = dict(((r,c),0) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
        "    \n",
        "    def update_action(self, hint):\n",
        "        #Update human movement, update state and reward\n",
        "        #print(hint, action)\n",
        "        if(hint == 0):\n",
        "            self.time_since_last_hint += 1      \n",
        "        else:\n",
        "            self.time_since_last_hint = 0\n",
        "        \n",
        "        action = self.human.move()\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward(hint)\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        env_state = self.observe()\n",
        "        return env_state, reward, status\n",
        "    \n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        self.human_loc, human.emotions, self.hint_on_display = self.state\n",
        "        row, col = self.human_loc\n",
        "        \n",
        "        if self.maze[self.human_loc] > 0.0:\n",
        "            self.visited[human.loc] += 1\n",
        "        if human in self.targets:\n",
        "            self.targets.remove(self.human_loc)\n",
        "            \n",
        "        if(self.hint == 1 or self.time_since_last_hint <= 5):\n",
        "            self.hint_on_display = 1 #True\n",
        "            \n",
        "        #Update movement\n",
        "        #Each step is 2 squares in VR Environment\n",
        "        if action == 1:\n",
        "            col -= 2\n",
        "        elif action == 2:\n",
        "            row -= 2\n",
        "        elif action == 3:\n",
        "            col += 2\n",
        "        elif action == 3:\n",
        "            row += 2\n",
        "            \n",
        "        self.human_loc = (row, col)\n",
        "        self.state = (self.human_loc, self.human.emotions, self.hint_on_display)\n",
        "    \n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        env_state = canvas.reshape((1, -1))\n",
        "        return env_state\n",
        "    \n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self._maze)\n",
        "        # draw the targets\n",
        "        for r,c in self.targets:\n",
        "            canvas[r,c] = target_mark\n",
        "        # draw the human\n",
        "        canvas[self.human_loc] = human_mark\n",
        "        return canvas\n",
        "    \n",
        "    def get_reward(self, hint = 0):\n",
        "        reward = 0\n",
        "        \n",
        "        if (self.hint_on_display == 1):\n",
        "            return self.min_reward\n",
        "        \n",
        "        #Get reward based on past frustration value\n",
        "        f = self.human.df_Emotions.loc['F']\n",
        "        y = f[len(f) - self.base : len(f)]\n",
        "        y.reverse()\n",
        "        for ix, item in enumerate(y):\n",
        "            reward = reward + item/((ix+1)*self.base)\n",
        "        \n",
        "        #Get reward based on change in frustration values\n",
        "        for i in range(len(y)-1):\n",
        "            reward = reward +  (y[i]- y[i+1])/(i+1)\n",
        "        \n",
        "        #If hint is given\n",
        "        if hint == 1:\n",
        "            reward = reward + self.reward['hint_penalty'] + self.reward['hint_time_penalty']\n",
        "            \n",
        "        #Reward based on targets collected and time spent\n",
        "        if len(self.targets) == 0:\n",
        "            reward =  reward + 1.0 + self.reward['targets']\n",
        "        elif self.human_loc in self.targets:\n",
        "            reward =  reward + self.reward['targets']\n",
        "        else:\n",
        "            reward =  reward + self.reward['time_penalty'] \n",
        "            \n",
        "        return reward\n",
        "        \n",
        "        \n",
        "    #Returns dictionary of cells with count of times visited\n",
        "    def get_visited_cells(self):\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        visited = dict(((r,c),0) \n",
        "             for r in range(nrows) \n",
        "             for c in range(ncols) if self._maze[r,c] == 1.0)\n",
        "        return visited\n",
        "  \n",
        "    #Returns numpy array of cells of maze in which movement is not allowed (walls)\n",
        "    def get_blocked_cells(self):\n",
        "        nrows, ncols = self._maze.shape\n",
        "        blocked_cells =[[r,c] for r in range(nrows) for c in range(ncols) if maze1[r,c] == 0.0]\n",
        "        return np.asarray(blocked_cells)\n",
        "    \n",
        "    #Returns the numpy array of cells in which free movement is allowed\n",
        "    def get_free_cells(self):\n",
        "        nrows, ncols = self._maze.shape\n",
        "        free_cells = [[r,c] for r in range(nrows) for c in range(ncols) if maze1[r,c] == 1.0]\n",
        "        return np.asarray(free_cells)\n",
        "    \n",
        "    #Returns valid actions from a certain location\n",
        "    def valid_actions(self, cell = None):\n",
        "        if cell is None:\n",
        "            row, col = self.human_loc\n",
        "        else:\n",
        "            row, col = cell\n",
        "        nrows, ncols = self.maze.shape\n",
        "        actions = [0, 1, 2, 3, 4]\n",
        "        \n",
        "        if row == 0:\n",
        "            actions.remove(2)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(4)\n",
        "            \n",
        "        if col == 0:\n",
        "            actions.remove(1)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(3)\n",
        "        \n",
        "        if row > 0 and self.maze[row-1, col] == 0:\n",
        "            actions.remove(2)\n",
        "        if row < nrows-1 and self.maze[row+1, col] == 0:\n",
        "            actions.remove(4)\n",
        "        if col>0 and self.maze[row, col-1] == 0:\n",
        "            actions.remove(1)\n",
        "        if col<ncols-1 and self.maze[row, col+1] ==0:\n",
        "            actions.remove(3) \n",
        "            \n",
        "        return actions\n",
        "    \n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        human.loc, human.emotions, mode = self.state\n",
        "        if len(self.targets) == 0 :\n",
        "            return 'win'\n",
        "        return 'ongoing'\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKLgcVNs8pfI",
        "colab_type": "text"
      },
      "source": [
        "## Human Class\n",
        "**Used to simulate the actions of human and generate corresponding EEG signals.** <br>\n",
        "**Used to fetch emotions from EEG Headset in case of actual experiments.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UrO2tui8pfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Human:\n",
        "    def __init__(self, maze, n = 4, curr_emotion = [0, 0, 0, 0, 0]):\n",
        "        self.emotions = curr_emotion\n",
        "        self.df_Emotions = pd.DataFrame(columns = EMOTIONS_LIST)\n",
        "        self.maze = maze\n",
        "        self.n_items = n\n",
        "        \n",
        "    def reset(self, emotions = [0, 0, 0, 0, 0]):\n",
        "        self.emotions = emotions\n",
        "        self.loc = START_LOC\n",
        "        self.items_left = self.n_items\n",
        "    \n",
        "    #Valid action from human perspective\n",
        "    #Returns valid actions from a certain location\n",
        "    def valid_actions(self):\n",
        "        if cell is None:\n",
        "            row, col = self.loc\n",
        "        else:\n",
        "            row, col = cell\n",
        "        nrows, ncols = self.maze.shape\n",
        "        actions = [0, 1, 2, 3, 4]\n",
        "        \n",
        "        if row == 0:\n",
        "            actions.remove(2)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(4)\n",
        "            \n",
        "        if col == 0:\n",
        "            actions.remove(1)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(3)\n",
        "        \n",
        "        if row > 0 and self.maze[row-1, col] == 0:\n",
        "            actions.remove(2)\n",
        "        if row < nrows-1 and self.maze[row+1, col] == 0:\n",
        "            actions.remove(4)\n",
        "        if col>0 and self.maze[row, col-1] == 0:\n",
        "            actions.remove(1)\n",
        "        if col<ncols-1 and self.maze[row, col+1] ==0:\n",
        "            actions.remove(3) \n",
        "            \n",
        "        return actions\n",
        "    \n",
        "    \n",
        "    def move(self, hint = 0):\n",
        "        if hint == 0:\n",
        "            #Random walk\n",
        "            move = random.choice(self.valid_actions)\n",
        "        else:\n",
        "            valid_moves = self.valid_actions()\n",
        "            #Estimate direction to move based on hint\n",
        "            x = np.array(self._curr_loc) - np.array(target_loc)\n",
        "            y = []\n",
        "            if (x[0]>1 ):\n",
        "                y.append(2)\n",
        "            if (x[0]<-1):\n",
        "                y.append(4)\n",
        "            if (x[1]>1):\n",
        "                y.append(1)\n",
        "            if (x[1]<-1):\n",
        "                y.append(3)\n",
        "            \n",
        "            possible_move = list(set(actions).intersection(set(y)))\n",
        "            \n",
        "            if( len(possible_move) == 0):\n",
        "                move = random.choice(valid_moves)\n",
        "            else:\n",
        "                move = random.choice(possible_move)\n",
        "                \n",
        "        #Update location\n",
        "        row, col = self.loc\n",
        "        if action == 1:\n",
        "            col -= 2\n",
        "        elif action == 2:\n",
        "            row -= 2\n",
        "        elif action == 3:\n",
        "            col += 2\n",
        "        elif action == 3:\n",
        "            row += 2\n",
        "        self.loc = (row, col)\n",
        "        #\n",
        "        if np.array_equal(self.loc, ITEMS_LOC(self.items_left)):\n",
        "            self.items_left -= 1\n",
        "        \n",
        "        return move\n",
        "    \n",
        "    def simulate_emotions(self):\n",
        "    #Simulate\n",
        "        self.emotions = np.array(random.sample(range(0,10), 3)) * 0.1\n",
        "        self.df_Emotion = self.df_Emotion.append(pd.Series(df_row, index = df_Emotion.columns), ignore_index = True)\n",
        "\n",
        "    def fetch_emotions():\n",
        "    #Get emotions from EEG when connected\n",
        "        self.emotions = np.array(random.sample(range(0,10), 3)) * 0.1\n",
        "        self.df_Emotion = self.df_Emotion.append(pd.Series(df_row, index = df_Emotion.columns), ignore_index = True)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsFZVHg38pfM",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning Experience Replay Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOzp63-X8pfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=MAX_MEMORY, discount=GAMMA):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, env_state):\n",
        "        return self.model.predict(env_state)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # env_state 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        labels = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
        "            inputs[i] = env_state\n",
        "            labels[i] = self.predict(env_state)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(next_env_state))\n",
        "            if game_over:\n",
        "                labels[i, action] = reward\n",
        "            else:\n",
        "                labels[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16u2EzQ08pfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qtraining():\n",
        "    def __init__(self, model, env, human, n = N_EPOCH, start = START_LOC):\n",
        "        self.model = model #NN model\n",
        "        self.env = env     #Environment\n",
        "        self.human = human\n",
        "        self.n_epoch = n\n",
        "        self.max_memory = 4* self.env.maze.size\n",
        "        self.data_size = 0.75* self.env.maze.size\n",
        "        self.start_cell = start\n",
        "        self.init_hint = 0\n",
        "        self.win_count = 0\n",
        "        self.hint_list = []\n",
        "        self.action_list = []\n",
        "        self.human_path = []\n",
        "        self.item_collected = 0\n",
        "\n",
        "\n",
        "        #Initialize experience replay \n",
        "        self.experience = Experience(self.model, max_memory= self.max_memory)\n",
        "        \n",
        "    def train(self):\n",
        "        start_time = datetime.datetime.now()\n",
        "        self.seconds = 0\n",
        "        self.win_count = 0\n",
        "        self.curr_epoch = 0\n",
        "        for epoch in range(self.n_epoch):\n",
        "            self.epoch = epoch\n",
        "            self.curr_epoch += 1\n",
        "            self.loss = 0.0\n",
        "            human = self.start_cell\n",
        "            #print(all_targets)\n",
        "            self.env.reset(self.start_cell, self.init_hint)\n",
        "            game_over = False\n",
        "            self.env_state = self.env.observe()\n",
        "            self.n_episodes = 0\n",
        "            self.hint_list = []\n",
        "            self.action_list = []\n",
        "            #self.human_loc.append(human)\n",
        "            while not game_over:\n",
        "                game_over = self.play()\n",
        "\n",
        "            dt = datetime.datetime.now() - start_time\n",
        "            self.seconds = dt.total_seconds()\n",
        "            t = format_time(self.seconds)\n",
        "            #fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | Targets: {:d} | e: {:.3f} | time: {}\"\n",
        "            #print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.targets), self.epsilon(), t))\n",
        "            #print(str(env.total_reward) + str(env.targets) + str(env.state))\n",
        "            #print('Hints :' +str(self.hint_list))\n",
        "            #print('Actions :' +str(self.action_list))\n",
        "            #print('Actions :' +str(self.human_loc))\n",
        "\n",
        "\n",
        "            self.item_collected += (len(self.env._targets) - len(self.env.targets)) / 3\n",
        "\n",
        "            #if epoch > 2:\n",
        "            #    if self.completion_check():\n",
        "            #        print(\"Completed training at epoch: %d\" %(epoch, ))\n",
        "            #        break   \n",
        "\n",
        "            if (epoch+1)%2 == 0:\n",
        "                fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | Targets: {:d} | e: {:.3f} | time: {}\"\n",
        "                print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.targets), self.epsilon(), t))\n",
        "                print(str(env.total_reward) + str(env.targets) + str(env.state))\n",
        "                print('Hints :' +str(self.hint_list))\n",
        "                print('Actions :' +str(self.action_list))\n",
        "\n",
        "\n",
        "    def play(self):\n",
        "        hint = self.get_hint()\n",
        "        action = self.human.move(hint)\n",
        "        self.action_list.append(action)\n",
        "        prev_env_state = self.env_state\n",
        "        self.env_state, reward, game_status = self.env.update_action(hint, action)\n",
        "        #self.human_loc.append(self.env.human)\n",
        "        #print(game_status)\n",
        "        #print(env.total_reward)\n",
        "        if game_status == 'win':\n",
        "            self.win_count += 1\n",
        "            game_over = True\n",
        "        elif game_status == 'lose':\n",
        "            game_over = True\n",
        "        else:\n",
        "            game_over = False\n",
        "\n",
        "        #episode = [prev_env_sate, action, hint, reward, self.env_state, game_over]\n",
        "        episode = [prev_env_state, hint, reward, self.env_state, game_over]\n",
        "\n",
        "        self.experience.remember(episode)\n",
        "        self.n_episodes += 1\n",
        "\n",
        "        # Train model\n",
        "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
        "        epochs = int(self.env.base)\n",
        "        h = self.model.fit(\n",
        "            inputs,\n",
        "            targets,\n",
        "            epochs = epochs,\n",
        "            batch_size=16,\n",
        "            verbose=0,\n",
        "        )\n",
        "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
        "        return game_over\n",
        "        \n",
        "    def run_game(self, hint = 0):\n",
        "        self.env.reset(self.human, hint)\n",
        "        self.hint_list = []\n",
        "        self.action_list = []\n",
        "        env_state = self.env.observe()\n",
        "        while True:\n",
        "            #get next hint\n",
        "            q = self.model.predict(env_state)\n",
        "            hint = np.argmax(q[0])\n",
        "            action = self.human.move(hint)\n",
        "            self.hint_list.append(hint)\n",
        "            self.action_list.append(action)\n",
        "            prev_env_state = env_state\n",
        "            #Apply hint and action\n",
        "            env_state, reward, game_status = self.env.update_action(hint, action)\n",
        "            print('Hints :' +str(self.hint_list))\n",
        "            print('Actions :' +str(self.action_list))\n",
        "            if game_status == 'win':\n",
        "                return True\n",
        "            elif game_status == 'lose':\n",
        "                return False\n",
        "\n",
        "    def get_hint(self):\n",
        "        #Get next hint\n",
        "        valid_hints = HINTS\n",
        "        if np.random.rand()< self.epsilon():\n",
        "            hint = random.choice(valid_hints)\n",
        "        else:\n",
        "            q = self.experience.predict(self.env_state)\n",
        "            hint = np.argmax(q)\n",
        "        self.hint_list.append(hint)\n",
        "        return hint\n",
        "        \n",
        "    def epsilon(self):\n",
        "        #n = self.win_count\n",
        "        n = self.curr_epoch\n",
        "        high = 0.25\n",
        "        low = 0.05\n",
        "        e = low + (high - low) /  (1 + 0.1 * np.log(n) )\n",
        "        return e\n",
        "        \n",
        "    def completion_check(self):\n",
        "        if not self.run_game(self.start_cell, hint = 0):\n",
        "            return False\n",
        "        return True\n",
        "    \n",
        "    def save(self, name=\"\"):\n",
        "        # Save trained model weights and architecture, this will be used by the visualization code\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        h5file = 'model_%s.h5' % (name,)\n",
        "        json_file = 'model_%s.json' % (name,)\n",
        "        self.model.save_weights(h5file, overwrite=True)\n",
        "        with open(json_file, \"w\") as outfile:\n",
        "            json.dump(self.model.to_json(), outfile)\n",
        "        t = format_time(self.seconds)\n",
        "        print('files: %s, %s' % (h5file, json_file))\n",
        "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eMkinvC8pfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(env):\n",
        "    loss =  'mse'\n",
        "    a = opt.get('alpha', 0.24)\n",
        "    model = Sequential()\n",
        "    esize = env.maze.size\n",
        "    model.add(Dense(esize, input_shape = (esize,)))\n",
        "    model.add(LeakyReLU(alpha = a))\n",
        "    model.add(Dense(esize))\n",
        "    model.add(LeakyReLU(alpha = a))\n",
        "    model.add(Dense(2))\n",
        "    model.compile(optimizer = 'adam', loss = 'mse')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLpBaFJ78pfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze(maze, ITEMS_LOC)\n",
        "human = human"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSsqiKvz8pfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(env)\n",
        "\n",
        "qt = Qtraining(\n",
        "    model,\n",
        "    env,\n",
        "    n_epoch = 1000,\n",
        "    max_memory = 1000,\n",
        "    data_size = 256,\n",
        "    name = 'model_1'\n",
        ")\n",
        "\n",
        "qt.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}